{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Data in DataFrames HW\n",
    "\n",
    "\n",
    "#### Let's get started applying what we learned in the lecure!\n",
    "\n",
    "I've provided several questions below to help test and expand you knowledge from the code along lecture. So let's see what you've got!\n",
    "\n",
    "First create your spark instance as we need to do at the start of every project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-JOJHKCD:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DF_practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x289baeb4cd0>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DF_practice\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our Republican vs. Democrats Tweet DataFrame\n",
    "\n",
    "Attached to the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Rep_vs_Dem_tweets.csv\"\n",
    "df = spark.read.csv(path,inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92489"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataframe\n",
    "\n",
    "Extracted tweets from all of the representatives (latest 200 as of May 17th 2018)\n",
    "\n",
    "**Source:** https://www.kaggle.com/kapastor/democratvsrepublicantweets#ExtractedTweets.csv\n",
    "\n",
    "Use either .show() or .toPandas() check out the first view rows of the dataframe to get an idea of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress has allocated about $18…\"</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NALCABPolicy: Meeting with @RepDarrenSoto ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Party         Handle  \\\n",
       "0                            Democrat  RepDarrenSoto   \n",
       "1                            Democrat  RepDarrenSoto   \n",
       "2                            Democrat  RepDarrenSoto   \n",
       "3  Congress has allocated about $18…\"           None   \n",
       "4                            Democrat  RepDarrenSoto   \n",
       "\n",
       "                                               Tweet  \n",
       "0  Today, Senate Dems vote to #SaveTheInternet. P...  \n",
       "1  RT @WinterHavenSun: Winter Haven resident / Al...  \n",
       "2  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...  \n",
       "3                                               None  \n",
       "4  RT @NALCABPolicy: Meeting with @RepDarrenSoto ...  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prevent Truncation of view**\n",
    "\n",
    "If the view you produced above truncated some of the longer tweets, see if you can prevent that so you can read the whole tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tweet                                                                                                                                       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Today, Senate Dems vote to #SaveTheInternet. Proud to support similar #NetNeutrality legislation here in the House… https://t.co/n3tggDLU1L |\n",
      "|RT @WinterHavenSun: Winter Haven resident / Alta Vista teacher is one of several recognized by @RepDarrenSoto for National Teacher Apprecia…|\n",
      "|RT @NBCLatino: .@RepDarrenSoto noted that Hurricane Maria has left approximately $90 billion in damages.                                    |\n",
      "|null                                                                                                                                        |\n",
      "|RT @NALCABPolicy: Meeting with @RepDarrenSoto . Thanks for taking the time to meet with @LatinoLeader ED Marucci Guzman. #NALCABPolicy2018.…|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Tweet).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Schema**\n",
    "\n",
    "First, check the schema to make sure the datatypes are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Party: string (nullable = true)\n",
      " |-- Handle: string (nullable = true)\n",
      " |-- Tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Can you identify any tweet that mentions the handle @LatinoLeader using regexp_extract?\n",
    "\n",
    "It doesn't matter how you identify the row, any identifier will do. You can test your script on row 5 from this dataset. That row contains @LatinoLeader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Party   |Handle       |Tweet                                                                                                                                       |\n",
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Democrat|RepDarrenSoto|RT @NALCABPolicy: Meeting with @RepDarrenSoto . Thanks for taking the time to meet with @LatinoLeader ED Marucci Guzman. #NALCABPolicy2018.…|\n",
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * #regexp_extract\n",
    "\n",
    "\n",
    "\n",
    "#method1\n",
    "df.select(\"*\").filter(df.Tweet.like(\"%@LatinoLeader%\")).show(5,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+\n",
      "|   Party|       Handle|               Tweet|\n",
      "+--------+-------------+--------------------+\n",
      "|Democrat|RepDarrenSoto|RT @NALCABPolicy:...|\n",
      "+--------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#method2... IDK\n",
    "\n",
    "\n",
    "df.filter(array_contains(split(\"Tweet\",\" \"),\"@LatinoLeader\") ==True).select(\"*\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Latino_Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congress has allocated about $18…\"</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NALCABPolicy: Meeting with @RepDarrenSoto ...</td>\n",
       "      <td>@LatinoLeader</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Party         Handle  \\\n",
       "0                            Democrat  RepDarrenSoto   \n",
       "1                            Democrat  RepDarrenSoto   \n",
       "2                            Democrat  RepDarrenSoto   \n",
       "3  Congress has allocated about $18…\"           None   \n",
       "4                            Democrat  RepDarrenSoto   \n",
       "\n",
       "                                               Tweet Latino_Mentions  \n",
       "0  Today, Senate Dems vote to #SaveTheInternet. P...                  \n",
       "1  RT @WinterHavenSun: Winter Haven resident / Al...                  \n",
       "2  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...                  \n",
       "3                                               None            None  \n",
       "4  RT @NALCABPolicy: Meeting with @RepDarrenSoto ...   @LatinoLeader  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method3 \n",
    "\n",
    "latino = df.withColumn(\"Latino_Mentions\",regexp_extract(df.Tweet,\"(.)(@LatinoLeader)(.)\",2))\n",
    "latino.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Replace any value other than 'Democrate' or 'Republican' with 'Other' in the Party column.\n",
    "\n",
    "We can see from the output below, that there are several other values other than 'Democrate' or 'Republican' in the Part column. We are assuming that this is dirty data that needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|Party                  |count|\n",
      "+-----------------------+-----+\n",
      "|Republican             |44392|\n",
      "|Democrat               |42068|\n",
      "|That’s…\"               |28   |\n",
      "|https://t.co/oc6JNAF5K5|22   |\n",
      "|Now                    |17   |\n",
      "|Today                  |13   |\n",
      "+-----------------------+-----+\n",
      "only showing top 6 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|               Party|count|\n",
      "+--------------------+-----+\n",
      "|          Republican|44392|\n",
      "|            Democrat|42068|\n",
      "|            That’s…\"|   28|\n",
      "|https://t.co/oc6J...|   22|\n",
      "|                 Now|   17|\n",
      "|               Today|   13|\n",
      "|         https://t…\"|   12|\n",
      "|              http…\"|   12|\n",
      "|                 h…\"|   12|\n",
      "|  ❌ Terminated #DACA|   11|\n",
      "|   ❌ Passed #TaxScam|   11|\n",
      "|        #mepolitics\"|   11|\n",
      "|               ❌ E…\"|   11|\n",
      "|❌ Abandoned Hispa...|   11|\n",
      "|      https://t.co…\"|    9|\n",
      "|https://t.co/8htz...|    9|\n",
      "|💻 Website: https...|    9|\n",
      "|This. Has. To. St...|    8|\n",
      "|Semper Fidelis. h...|    7|\n",
      "|                   4|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = df.groupBy(\"Party\").count()\n",
    "counts.orderBy(desc(\"count\")).show(6,False)\n",
    "#counts.orderBy(counts[\"count\"].desc()).show(6,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     Party|count|\n",
      "+----------+-----+\n",
      "|Republican|44392|\n",
      "|  Democrat|42068|\n",
      "|     Other| 6029|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "clean = df.withColumn(\"Party\", (when(df.Party==\"Democrat\",\"Democrat\").when(df.Party==\"Republican\",\"Republican\").otherwise(\"Other\")))\n",
    "counts = clean.groupBy(\"Party\").count()\n",
    "counts.orderBy(desc(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete all embedded links (ie. \"https:....)\n",
    "\n",
    "For example see the first row in the tweets dataframe. \n",
    "\n",
    "*Note: this may require an google search :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|Tweet                                                                                                                                      |regexp_replace(Tweet, (https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b, )                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|Today, Senate Dems vote to #SaveTheInternet. Proud to support similar #NetNeutrality legislation here in the House… https://t.co/n3tggDLU1L|Today, Senate Dems vote to #SaveTheInternet. Proud to support similar #NetNeutrality legislation here in the House… |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "df.select(df.Tweet,regexp_replace(df.Tweet,r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b',\"\")).show(1,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove any leading or trailing white space in the tweet column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Tweet|\n",
      "+--------------------+\n",
      "|Today, Senate Dem...|\n",
      "|RT @WinterHavenSu...|\n",
      "|RT @NBCLatino: .@...|\n",
      "|                null|\n",
      "|RT @NALCABPolicy:...|\n",
      "|RT @Vegalteno: Hu...|\n",
      "|RT @EmgageActionF...|\n",
      "|Hurricane Maria l...|\n",
      "|RT @Tharryry: I a...|\n",
      "|RT @HispanicCaucu...|\n",
      "|RT @RepStephMurph...|\n",
      "|RT @AllSaints_FL:...|\n",
      "|.@realDonaldTrump...|\n",
      "|Thank you to my m...|\n",
      "|We paid our respe...|\n",
      "|                null|\n",
      "|RT @WinterHavenSu...|\n",
      "|Meet 12 incredibl...|\n",
      "|RT @wildlifeactio...|\n",
      "|RT @CHeathWFTV: K...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim,ltrim,rtrim\n",
    "df.select(trim(df.Tweet).alias(\"Tweet\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rename the 'Party' column to 'Dem_Rep'\n",
    "\n",
    "No real reason here :) just wanted you to get practice doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|             Dem_Rep|       Handle|               Tweet|\n",
      "+--------------------+-------------+--------------------+\n",
      "|            Democrat|RepDarrenSoto|Today, Senate Dem...|\n",
      "|            Democrat|RepDarrenSoto|RT @WinterHavenSu...|\n",
      "|            Democrat|RepDarrenSoto|RT @NBCLatino: .@...|\n",
      "|Congress has allo...|         null|                null|\n",
      "|            Democrat|RepDarrenSoto|RT @NALCABPolicy:...|\n",
      "|            Democrat|RepDarrenSoto|RT @Vegalteno: Hu...|\n",
      "|            Democrat|RepDarrenSoto|RT @EmgageActionF...|\n",
      "|            Democrat|RepDarrenSoto|Hurricane Maria l...|\n",
      "|            Democrat|RepDarrenSoto|RT @Tharryry: I a...|\n",
      "|            Democrat|RepDarrenSoto|RT @HispanicCaucu...|\n",
      "|            Democrat|RepDarrenSoto|RT @RepStephMurph...|\n",
      "|            Democrat|RepDarrenSoto|RT @AllSaints_FL:...|\n",
      "|            Democrat|RepDarrenSoto|.@realDonaldTrump...|\n",
      "|            Democrat|RepDarrenSoto|Thank you to my m...|\n",
      "|            Democrat|RepDarrenSoto|We paid our respe...|\n",
      "|Sgt Sam Howard - ...|         null|                null|\n",
      "|            Democrat|RepDarrenSoto|RT @WinterHavenSu...|\n",
      "|            Democrat|RepDarrenSoto|Meet 12 incredibl...|\n",
      "|            Democrat|RepDarrenSoto|RT @wildlifeactio...|\n",
      "|            Democrat|RepDarrenSoto|RT @CHeathWFTV: K...|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.withColumnRenamed(\"Party\",\"Dem_Rep\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Concatenate the Party and Handle columns\n",
    "\n",
    "Silly yes... but good practice.\n",
    "\n",
    "pyspark.sql.functions.concat_ws(sep, *cols)[source] <br>\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|               Party|       Handle|              concon|\n",
      "+--------------------+-------------+--------------------+\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|Congress has allo...|         null|Congress has allo...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|Sgt Sam Howard - ...|         null|Sgt Sam Howard - ...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|Democrat RepDarre...|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------+---------------------------+\n",
      "|               Party|       Handle|concat_ws( , Party, Handle)|\n",
      "+--------------------+-------------+---------------------------+\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|Congress has allo...|         null|       Congress has allo...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|Sgt Sam Howard - ...|         null|       Sgt Sam Howard - ...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "|            Democrat|RepDarrenSoto|       Democrat RepDarre...|\n",
      "+--------------------+-------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df.select(\"Party\",\"Handle\",concat_ws(\" \",\"Party\",\"Handle\").alias(\"concon\")).show()\n",
    "\n",
    "#or\n",
    "\n",
    "df.select(\"Party\",\"Handle\",concat_ws(\" \",df.Party,df.Handle)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Question\n",
    "\n",
    "Let's imagine that we want to analyze the hashtags that are used in these tweets. Can you extract all the hashtags you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression!\n",
    "\n",
    "- the \" . \" maches any character other than a new line\n",
    "- \" | \" is similar to \"or\"\n",
    "- \\w+  means followed by any word\n",
    "\n",
    "- \" * \" is used to match the preceding character **zero or more times**\n",
    "- \" ? \" will match the preceding character **zero or one, but no more.**\n",
    "- \" $ \" is used to match the ending position in a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".*?((.|)(#)(\\w+)) (.*(.|)(#)(\\w+)).*?$\n"
     ]
    }
   ],
   "source": [
    "pattern = '(.|'')(#)(\\w+)'\n",
    "split_pattern = r'.*?({pattern})'.format(pattern=pattern)\n",
    "end_pattern = r'(.*{pattern}).*?$'.format(pattern=pattern)\n",
    "\n",
    "print(split_pattern,end_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             HashTag|\n",
      "+--------------------+\n",
      "|    #SaveTheInternet|\n",
      "|   #NALCABPolicy2018|\n",
      "|      #NetNeutrality|\n",
      "|            #Orlando|\n",
      "|             #HFAnow|\n",
      "|#classroomwhereit...|\n",
      "|               #FACT|\n",
      "|        #ABetterDeal|\n",
      "|            #Science|\n",
      "|         #2020Census|\n",
      "|          #ghostguns|\n",
      "|              #APAHM|\n",
      "|        #Insulin4all|\n",
      "|      #EmgagetheHill|\n",
      "|      #Emgagethehill|\n",
      "|                #TPS|\n",
      "|     #HurricaneMaria|\n",
      "|           #vamos4pr|\n",
      "|          #DemsAtWor|\n",
      "|         #DemsAtWork|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "# df.filter(df.Tweet.like(\"%#%\")).select(regexp_extract(df.Tweet,r\"(#[a-zA-Z0-9]*)\",0).alias(\"HashTag\")).show()\n",
    "\n",
    "df.filter(df.Tweet.like(\"%#%\")).select(regexp_extract(df.Tweet,r\"(#\\w*)\",0).alias(\"HashTag\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create our own dataset to work with real dates\n",
    "\n",
    "This is a dataset of patient visits from a medical office. It contains the patients' first and last names, date of birth, and the dates of their first 3 visits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+---------+---------+\n",
      "|first_name|last_name|       dob|  visit1|   visit2|   visit3|\n",
      "+----------+---------+----------+--------+---------+---------+\n",
      "|  Mohammed|   Alfasy|  1987-4-8|2016-1-7| 2017-2-3| 2018-3-2|\n",
      "|     Marcy|Wellmaker|  1986-4-8|2015-1-7| 2017-1-3| 2018-1-2|\n",
      "|     Ginny|   Ginger| 1986-7-10|2014-8-7| 2015-2-3| 2016-3-2|\n",
      "|     Vijay| Doberson|  1988-5-2|2016-1-7| 2018-2-3| 2018-3-2|\n",
      "|     Orhan|  Gelicek| 1987-5-11|2016-5-7| 2017-1-3| 2018-9-2|\n",
      "|     Sarah|    Jones|  1956-7-6|2016-4-7| 2017-8-3|2018-10-2|\n",
      "|      John|  Johnson|2017-10-12|2018-1-2|2018-10-3| 2018-3-2|\n",
      "+----------+---------+----------+--------+---------+---------+\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- visit1: string (nullable = true)\n",
      " |-- visit2: string (nullable = true)\n",
      " |-- visit3: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "md_office = [('Mohammed','Alfasy','1987-4-8','2016-1-7','2017-2-3','2018-3-2') \\\n",
    "            ,('Marcy','Wellmaker','1986-4-8','2015-1-7','2017-1-3','2018-1-2') \\\n",
    "            ,('Ginny','Ginger','1986-7-10','2014-8-7','2015-2-3','2016-3-2') \\\n",
    "            ,('Vijay','Doberson','1988-5-2','2016-1-7','2018-2-3','2018-3-2') \\\n",
    "            ,('Orhan','Gelicek','1987-5-11','2016-5-7','2017-1-3','2018-9-2') \\\n",
    "            ,('Sarah','Jones','1956-7-6','2016-4-7','2017-8-3','2018-10-2') \\\n",
    "            ,('John','Johnson','2017-10-12','2018-1-2','2018-10-3','2018-3-2') ]\n",
    "\n",
    "df = spark.createDataFrame(md_office,['first_name','last_name','dob','visit1','visit2','visit3']) # schema=final_struc\n",
    "\n",
    "# Check to make sure it worked\n",
    "df.show()\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! The dates are still stored as text... let's try converting them again and see if we have any issues this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>dob</th>\n",
       "      <th>visit1</th>\n",
       "      <th>visit2</th>\n",
       "      <th>visit3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mohammed</td>\n",
       "      <td>Alfasy</td>\n",
       "      <td>1987-04-08</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>2018-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marcy</td>\n",
       "      <td>Wellmaker</td>\n",
       "      <td>1986-04-08</td>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ginny</td>\n",
       "      <td>Ginger</td>\n",
       "      <td>1986-07-10</td>\n",
       "      <td>2014-08-07</td>\n",
       "      <td>2015-02-03</td>\n",
       "      <td>2016-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vijay</td>\n",
       "      <td>Doberson</td>\n",
       "      <td>1988-05-02</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>2018-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orhan</td>\n",
       "      <td>Gelicek</td>\n",
       "      <td>1987-05-11</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>2018-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Jones</td>\n",
       "      <td>1956-07-06</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>2018-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>John</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>2018-03-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name  last_name         dob      visit1      visit2      visit3\n",
       "0   Mohammed     Alfasy  1987-04-08  2016-01-07  2017-02-03  2018-03-02\n",
       "1      Marcy  Wellmaker  1986-04-08  2015-01-07  2017-01-03  2018-01-02\n",
       "2      Ginny     Ginger  1986-07-10  2014-08-07  2015-02-03  2016-03-02\n",
       "3      Vijay   Doberson  1988-05-02  2016-01-07  2018-02-03  2018-03-02\n",
       "4      Orhan    Gelicek  1987-05-11  2016-05-07  2017-01-03  2018-09-02\n",
       "5      Sarah      Jones  1956-07-06  2016-04-07  2017-08-03  2018-10-02\n",
       "6       John    Johnson  2017-10-12  2018-01-02  2018-10-03  2018-03-02"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import * \n",
    "\n",
    "df = df.withColumn('dob',df['dob'].cast(DateType()))\\\n",
    "        .withColumn(\"visit1\",df.visit1.cast(DateType()))\\\n",
    "        .withColumn(\"visit2\",df.visit2.cast(DateType()))\\\n",
    "        .withColumn(\"visit3\",df.visit3.cast(DateType()))\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Can you calculate a variable showing the length of time between patient visits?\n",
    "\n",
    "Compare visit1 to visit2 and visit2 to visit3 for all patients and see what the average length of time is between visits. Create an alias for it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------+\n",
      "|between first and second|between second and third|\n",
      "+------------------------+------------------------+\n",
      "|       -1 years -27 days|       -1 years -27 days|\n",
      "|    -1 years -11 mont...|     -11 months -30 days|\n",
      "|      -5 months -27 days|       -1 years -27 days|\n",
      "|       -2 years -27 days|                -27 days|\n",
      "|      -7 months -27 days|    -1 years -7 month...|\n",
      "|    -1 years -3 month...|    -1 years -1 month...|\n",
      "|       -9 months -1 days|         7 months 1 days|\n",
      "+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#In the form of years and days.\n",
    "df.select((df.visit1-df.visit2)\\\n",
    "          .alias(\"between first and second\"),(df.visit2-df.visit3)\\\n",
    "          .alias(\"between second and third\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "| 393|\n",
      "| 727|\n",
      "| 180|\n",
      "| 758|\n",
      "| 241|\n",
      "| 483|\n",
      "| 274|\n",
      "| 392|\n",
      "| 364|\n",
      "| 393|\n",
      "|  27|\n",
      "| 607|\n",
      "| 425|\n",
      "|-215|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "#numbers in days\n",
    "\n",
    "diff1 = df.select(datediff(df.visit2,df.visit1).alias(\"diff\"))\n",
    "diff2= df.select(datediff(df.visit3,df.visit2).alias(\"diff\"))\n",
    "\n",
    "#Append the two dataframes together\n",
    "\n",
    "diff_combo = diff1.union(diff2)\n",
    "diff_combo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|avg(CAST(regexp_replace(CAST((to_timestamp(`visit2`) - to_timestamp(`visit1`)) AS STRING),  hours, ) AS INT))|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                           10477.714285714286|\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(regexp_replace((to_timestamp(df.visit2)-to_timestamp(df.visit1)).cast(StringType()),\" hours\",\"\").cast(IntegerType()))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Can you calculate the age of each patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|format_number((datediff(current_date(), dob) / 365), 1)|\n",
      "+-------------------------------------------------------+\n",
      "|                                                   34.4|\n",
      "|                                                   35.4|\n",
      "|                                                   35.2|\n",
      "|                                                   33.4|\n",
      "|                                                   34.4|\n",
      "|                                                   65.2|\n",
      "|                                                    3.9|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "#1\n",
    "df.select(format_number(datediff(current_date(),df.dob)/365,1)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|           name|(current_date() - dob)|\n",
      "+---------------+----------------------+\n",
      "|Mohammed Alfasy|  34 years 5 months...|\n",
      "|Marcy Wellmaker|  35 years 5 months...|\n",
      "|   Ginny Ginger|  35 years 1 months...|\n",
      "| Vijay Doberson|  33 years 4 months...|\n",
      "|  Orhan Gelicek|  34 years 3 months...|\n",
      "|    Sarah Jones|  65 years 2 months...|\n",
      "|   John Johnson|  3 years 10 months...|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df.select(concat_ws(\" \",df.first_name, df.last_name)\\\n",
    "          .alias(\"name\"),current_date() - df.dob).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Can you extract the month from the first visit column and call it \"Month\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|   01|\n",
      "|   01|\n",
      "|   08|\n",
      "|   01|\n",
      "|   05|\n",
      "|   04|\n",
      "|   01|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 regular expression\n",
    "df.select(regexp_extract(df.visit1,\"([0-9]*)-([0-9]*)-([0-9]*)\",2)\\\n",
    "          .alias(\"Month\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    8|\n",
      "|    1|\n",
      "|    5|\n",
      "|    4|\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "#2month function\n",
    "df.select(month(df['visit1']).alias(\"Month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Challenges with working with date and timestamps\n",
    "\n",
    "Let's read in the supermarket sales dataframe attached to the lecture now and see some of the issues that can come up when working with date and timestamps values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path=\"../data/supermarket_sales.csv\"\n",
    "df = spark.read.csv(path,inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "The growth of supermarkets in most populated cities are increasing and market competitions are also high. The dataset is one of the historical sales of supermarket company which has recorded in 3 different branches for 3 months data. \n",
    "\n",
    " - Attribute information\n",
    " - Invoice id: Computer generated sales slip invoice identification number\n",
    " - Branch: Branch of supercenter (3 branches are available identified by A, B and C).\n",
    " - City: Location of supercenters\n",
    " - Customer type: Type of customers, recorded by Members for customers using member card and Normal for without member card.\n",
    " - Gender: Gender type of customer\n",
    " - Product line: General item categorization groups - Electronic accessories, Fashion accessories, Food and beverages, Health and beauty, Home and lifestyle, Sports and travel\n",
    " - Unit price: Price of each product in USD\n",
    " - Quantity: Number of products purchased by customer\n",
    " - Tax: 5% tax fee for customer buying\n",
    " - Total: Total price including tax\n",
    " - Date: Date of purchase (Record available from January 2019 to March 2019)\n",
    " - Time: Purchase time (10am to 9pm)\n",
    " - Payment: Payment used by customer for purchase (3 methods are available – Cash, Credit card and Ewallet)\n",
    " - COGS: Cost of goods sold\n",
    " - Gross margin percentage: Gross margin percentage\n",
    " - Gross income: Gross income\n",
    " - Rating: Customer stratification rating on their overall shopping experience (On a scale of 1 to 10)\n",
    "\n",
    "**Source:** https://www.kaggle.com/aungpyaeap/supermarket-sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View dataframe and schema as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|Invoice ID |Branch|City     |Customer type|Gender|Product line          |Unit price|Quantity|Tax 5% |Total   |Date     |Time |Payment    |cogs  |gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|A     |Yangon   |Member       |Female|Health and beauty     |74.69     |7       |26.1415|548.9715|1/5/2019 |13:08|Ewallet    |522.83|4.761904762            |26.1415     |9.1   |\n",
      "|226-31-3081|C     |Naypyitaw|Normal       |Female|Electronic accessories|15.28     |5       |3.82   |80.22   |3/8/2019 |10:29|Cash       |76.4  |4.761904762            |3.82        |9.6   |\n",
      "|631-41-3108|A     |Yangon   |Normal       |Male  |Home and lifestyle    |46.33     |7       |16.2155|340.5255|3/3/2019 |13:23|Credit card|324.31|4.761904762            |16.2155     |7.4   |\n",
      "|123-19-1176|A     |Yangon   |Member       |Male  |Health and beauty     |58.22     |8       |23.288 |489.048 |1/27/2019|20:33|Ewallet    |465.76|4.761904762            |23.288      |8.4   |\n",
      "+-----------+------+---------+-------------+------+----------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice ID: string (nullable = true)\n",
      " |-- Branch: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Customer type: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Unit price: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Tax 5%: double (nullable = true)\n",
      " |-- Total: double (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      " |-- cogs: double (nullable = true)\n",
      " |-- gross margin percentage: double (nullable = true)\n",
      " |-- gross income: double (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date field to date type\n",
    "\n",
    "Looks like we need to convert the date field into a date type. Let's go ahead and do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that this method gives all null values\n",
      "        Date Formatted Date\n",
      "0   1/5/2019           None\n",
      "1   3/8/2019           None\n",
      "2   3/3/2019           None\n",
      "3  1/27/2019           None\n",
      "4   2/8/2019           None\n",
      "5  3/25/2019           None\n"
     ]
    }
   ],
   "source": [
    "print(\"Note that this method gives all null values\")\n",
    "df = df.withColumn(\"Formatted Date\", df[\"Date\"].cast(DateType()))\n",
    "df = df.select(\"Date\",\"Formatted Date\")\n",
    "print(df.limit(6).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "This result gives the wrong results (notice that all months are january)\n",
      "+---------+-------------+-----+\n",
      "|     Date|Dateformatted|Month|\n",
      "+---------+-------------+-----+\n",
      "| 1/5/2019|   2019-01-05|    1|\n",
      "| 3/8/2019|   2019-01-08|    1|\n",
      "| 3/3/2019|   2019-01-03|    1|\n",
      "|1/27/2019|   2019-01-27|    1|\n",
      "| 2/8/2019|   2019-01-08|    1|\n",
      "+---------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" \")\n",
    "print(\"This result gives the wrong results (notice that all months are january)\")\n",
    "df.select('Date',to_date(df.Date, 'm/d/yyyy').alias('Dateformatted'),month(to_date(df.Date, 'm/d/yyyy')).alias('Month'),).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But if we capitalize the mm part in the format, we get the correct results!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Dateformatted</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/5/2019</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/8/2019</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/27/2019</td>\n",
       "      <td>2019-01-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/8/2019</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1/29/2019</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3/2/2019</td>\n",
       "      <td>2019-03-02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2/9/2019</td>\n",
       "      <td>2019-02-09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2/22/2019</td>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2/18/2019</td>\n",
       "      <td>2019-02-18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Dateformatted  Month\n",
       "0     1/5/2019    2019-01-05      1\n",
       "1     3/8/2019    2019-03-08      3\n",
       "2     3/3/2019    2019-03-03      3\n",
       "3    1/27/2019    2019-01-27      1\n",
       "4     2/8/2019    2019-02-08      2\n",
       "..         ...           ...    ...\n",
       "995  1/29/2019    2019-01-29      1\n",
       "996   3/2/2019    2019-03-02      3\n",
       "997   2/9/2019    2019-02-09      2\n",
       "998  2/22/2019    2019-02-22      2\n",
       "999  2/18/2019    2019-02-18      2\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"But if we capitalize the mm part in the format, we get the correct results!\")\n",
    "df.select('Date',to_date(df.Date, 'M/d/yyyy').alias('Dateformatted'),month(to_date(df.Date, 'M/d/yyyy')).alias('Month'),).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>converted time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/5/2019</td>\n",
       "      <td>2019-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/8/2019</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>2019-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/27/2019</td>\n",
       "      <td>2019-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/8/2019</td>\n",
       "      <td>2019-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1/29/2019</td>\n",
       "      <td>2019-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3/2/2019</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2/9/2019</td>\n",
       "      <td>2019-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2/22/2019</td>\n",
       "      <td>2019-02-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2/18/2019</td>\n",
       "      <td>2019-02-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date converted time\n",
       "0     1/5/2019     2019-01-05\n",
       "1     3/8/2019     2019-03-08\n",
       "2     3/3/2019     2019-03-03\n",
       "3    1/27/2019     2019-01-27\n",
       "4     2/8/2019     2019-02-08\n",
       "..         ...            ...\n",
       "995  1/29/2019     2019-01-29\n",
       "996   3/2/2019     2019-03-02\n",
       "997   2/9/2019     2019-02-09\n",
       "998  2/22/2019     2019-02-22\n",
       "999  2/18/2019     2019-02-18\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UDF! \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "date_udf = udf(lambda x: datetime.strptime(x, \"%m/%d/%Y\"), DateType())\n",
    "\n",
    "df.select(\"Date\",date_udf(\"Date\").alias(\"converted time\")).toPandas() #this seems right!\n",
    "\n",
    "\n",
    "#It is noteworthy, that in order to parse through a 2-digit year, \n",
    "#e.g. '90' rather than '1990', a %y is required instead of a %Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we extract the month value from the date field?\n",
    "\n",
    "If you had trouble converting the date field in the previous question think about a more creative solution to extract the month from that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Date|Month|\n",
      "+---------+-----+\n",
      "| 1/5/2019|    1|\n",
      "| 3/8/2019|    3|\n",
      "| 3/3/2019|    3|\n",
      "|1/27/2019|    1|\n",
      "| 2/8/2019|    2|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('Date',split(df.Date, '/')[0].alias('Month'))\n",
    "\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0 Working with Arrays\n",
    "\n",
    "Here is a dataframe of reviews from the movie the Dark Night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------+\n",
      "|rating|review_txt                                                                              |\n",
      "+------+----------------------------------------------------------------------------------------+\n",
      "|5     |Epic. This is the best movie I have EVER seen                                           |\n",
      "|4     |Pretty good, but I would have liked to seen better special effects                      |\n",
      "|3     |So so. Casting could have been improved                                                 |\n",
      "|5     |The most EPIC movie of the year! Casting was awesome. Special effects were so intense.  |\n",
      "|4     |Solid but I would have liked to see more of the love story                              |\n",
      "|5     |THE BOMB!!!!!!!                                                                         |\n",
      "+------+----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "values = [(5,'Epic. This is the best movie I have EVER seen'), \\\n",
    "          (4,'Pretty good, but I would have liked to seen better special effects'), \\\n",
    "          (3,'So so. Casting could have been improved'), \\\n",
    "          (5,'The most EPIC movie of the year! Casting was awesome. Special effects were so intense.  '), \\\n",
    "          (4,'Solid but I would have liked to see more of the love story  '), \\\n",
    "          (5,'THE BOMB!!!!!!!')]\n",
    "reviews = spark.createDataFrame(values,['rating', 'review_txt'])\n",
    "\n",
    "reviews.show(6,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Let's see if we can create an array off of the review text column and then derive some meaningful results from it.\n",
    "\n",
    "**But first** we need to clean the rview_txt column to make sure we can get what we need from our analysis later on. So let's do the following:\n",
    "\n",
    "1. Remove all punctuation\n",
    "2. lower case everything\n",
    "3. Remove white space (trim)\n",
    "3. Then finally, split the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|renewed                                                                              |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|Epic This is the best movie I have EVER seen                                         |\n",
      "|Pretty good but I would have liked to seen better special effects                    |\n",
      "|So so Casting could have been improved                                               |\n",
      "|The most EPIC movie of the year Casting was awesome Special effects were so intense  |\n",
      "|Solid but I would have liked to see more of the love story                           |\n",
      "|THE BOMB                                                                             |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 remove all punctuation\n",
    "reviews.select(regexp_replace(reviews.review_txt,r\"[./,!]\",\"\").alias(\"renewed\")).show(6,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|<lambda>(review_txt)|\n",
      "+--------------------+\n",
      "|epic. this is the...|\n",
      "|pretty good, but ...|\n",
      "|so so. casting co...|\n",
      "|the most epic mov...|\n",
      "|solid but i would...|\n",
      "|     the bomb!!!!!!!|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 lower case everything\n",
    "#2-1\n",
    "udf_lower = udf(lambda x: x.lower(),StringType())\n",
    "reviews.select(udf_lower(reviews.review_txt)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   lower(review_txt)|\n",
      "+--------------------+\n",
      "|epic. this is the...|\n",
      "|pretty good, but ...|\n",
      "|so so. casting co...|\n",
      "|the most epic mov...|\n",
      "|solid but i would...|\n",
      "|     the bomb!!!!!!!|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2-2\n",
    "reviews.select(lower(reviews.review_txt)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>len-trimmed</th>\n",
       "      <th>len-untrimmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Epic. This is the best movie I have EVER seen</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretty good, but I would have liked to seen be...</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So so. Casting could have been improved</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most EPIC movie of the year! Casting was a...</td>\n",
       "      <td>86</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solid but I would have liked to see more of th...</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>THE BOMB!!!!!!!</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Trimmed  len-trimmed  \\\n",
       "0      Epic. This is the best movie I have EVER seen           45   \n",
       "1  Pretty good, but I would have liked to seen be...           66   \n",
       "2            So so. Casting could have been improved           39   \n",
       "3  The most EPIC movie of the year! Casting was a...           86   \n",
       "4  Solid but I would have liked to see more of th...           58   \n",
       "5                                    THE BOMB!!!!!!!           15   \n",
       "\n",
       "   len-untrimmed  \n",
       "0             45  \n",
       "1             66  \n",
       "2             39  \n",
       "3             88  \n",
       "4             60  \n",
       "5             15  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 Remove white space (trim)\n",
    "\n",
    "reviews.select(trim(reviews.review_txt).alias(\"Trimmed\"),length(trim(reviews.review_txt)).alias(\"len-trimmed\"),length(reviews.review_txt).alias(\"len-untrimmed\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|Final                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|[epic, this, is, the, best, movie, i, have, ever, seen]                                            |\n",
      "|[pretty, good, but, i, would, have, liked, to, seen, better, special, effects]                     |\n",
      "|[so, so, casting, could, have, been, improved]                                                     |\n",
      "|[the, most, epic, movie, of, the, year, casting, was, awesome, special, effects, were, so, intense]|\n",
      "|[solid, but, i, would, have, liked, to, see, more, of, the, love, story]                           |\n",
      "|[the, bomb]                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Then finally, split the string\n",
    "reviews.select(split(regexp_replace(lower(trim(reviews.review_txt)),r\"[./,!]\",\"\"),\" \").alias(\"Final\")).show(20,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Alright now let's see if we can find which reviews contain the word 'Epic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Things to check</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[epic, this, is, the, best, movie, i, have, ev...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pretty, good, but, i, would, have, liked, to,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[so, so, casting, could, have, been, improved]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, most, epic, movie, of, the, year, castin...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[solid, but, i, would, have, liked, to, see, m...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[the, bomb]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Things to check  Final\n",
       "0  [epic, this, is, the, best, movie, i, have, ev...   True\n",
       "1  [pretty, good, but, i, would, have, liked, to,...  False\n",
       "2     [so, so, casting, could, have, been, improved]  False\n",
       "3  [the, most, epic, movie, of, the, year, castin...   True\n",
       "4  [solid, but, i, would, have, liked, to, see, m...  False\n",
       "5                                        [the, bomb]  False"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.select(split(regexp_replace(lower(trim(reviews.review_txt)),r\"[./,!]\",\"\"),\" \").alias(\"Things to check\"),array_contains(split(regexp_replace(lower(trim(reviews.review_txt)),r\"[./,!]\",\"\"),\" \"),\"epic\").alias(\"Final\")).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
