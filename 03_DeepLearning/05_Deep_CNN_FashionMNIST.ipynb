{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Deep_CNN_FashionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7T_W572N1_r"
      },
      "source": [
        "### module configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfungahXN08M"
      },
      "source": [
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import pdb\n",
        "import torch.nn as nn"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qwv7vOFONfj"
      },
      "source": [
        "### device -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0E5y7hhOLf0",
        "outputId": "d1739048-4a8e-4bcb-c708-0332731821b1"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys-0QcIoOher"
      },
      "source": [
        "### hyperparameter 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbOJuPf2OLur"
      },
      "source": [
        "num_epoch = 5\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "batch_size = 100\n",
        "#batch 해야하는 이유?\n",
        "#로스 조정을 여러번하기 위함.\n",
        "#컴퓨터 부하를 줄이기 위함(병렬처리 하니까)\n",
        "\n",
        "num_classes = 10 \n",
        "#답이 열개 \n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkyxjXwtPGPY"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptMQY9_1OL18"
      },
      "source": [
        "#step1\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(download=True,\n",
        "                                                  root=\"../../data\",\n",
        "                                                  train=True, #트레이닝 데이터라는 의미\n",
        "                                                  transform = transforms.ToTensor() #이미지 변환, 스케일링, 채널 변경\n",
        "                                                  )\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "                                                  root=\"../../data\",\n",
        "                                                  train=False, #테스트 데이터라는 의미\n",
        "                                                  transform = transforms.ToTensor() #이미지 변환, 스케일링, 채널 변경\n",
        "                                                  )\n",
        "\n",
        "#step2 \n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(batch_size=batch_size,\n",
        "                                               shuffle = True,\n",
        "                                               dataset = train_dataset\n",
        "                                               )\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(batch_size=batch_size,\n",
        "                                               shuffle = False,\n",
        "                                               dataset = test_dataset\n",
        "                                               )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmam_0R2P1m0"
      },
      "source": [
        "### Model Template\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5roXIQSGQPn6"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,num_classes): #정답의 갯수가 몇개인가를 선지정해준다. \n",
        "        super(ConvNet,self).__init__()\n",
        "        \n",
        "        #레이어 설정\n",
        "        self.layer1 = nn.Sequential(\n",
        "            #convolution \n",
        "            nn.Conv2d(1,16,kernel_size=3,padding=1,stride=1) , \n",
        "            #activation function\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.MaxPool2d(kernel_size=2,stride=2) # 14 \n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16,32,kernel_size=3,padding=1,stride=1) ,\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer4 = nn.MaxPool2d(kernel_size=2,stride=2) # 7\n",
        "\n",
        "        self.layer5 = nn.Linear(7*7*32,num_classes)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        #출력물\n",
        "        out = self.layer1(x) #x는 이미지 데이터.  #getter setter?\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        #reshape! \n",
        "        #pdb.set_trace()\n",
        "        out = out.reshape(out.size(0),-1) #.. \n",
        "        out = self.layer5(out)\n",
        "        return out\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFkvwOZZTntm"
      },
      "source": [
        "### 모델 선정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqaW4dQ-QPhC"
      },
      "source": [
        "model = ConvNet(num_classes).to(device) #GPU \n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdlPbrz2VMJG"
      },
      "source": [
        "### 학습(training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHJAPXurQPS6",
        "outputId": "3f2e993a-5333-4a20-89ec-037da8535047"
      },
      "source": [
        "total_step = len(train_dataloader) #배치사이즈의 수 만큼 돌리는 것을 하나의 epoch으로 볼 것이다.\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (images,labels) in enumerate(train_dataloader):\n",
        "        image = images.to(device) #이미지 연산을 GPU로... 병렬처리 해주겠다?\n",
        "        label = labels.to(device) # image가 gpu서버에서 연산되므로... 같이가자... \n",
        "\n",
        "        output_forward = model(image)\n",
        "\n",
        "        loss_w = loss_function(output_forward,label) \n",
        "        #정답과 비교 + 로스값 계산 + 양수화 시키고 증폭... log로 숫자 다시 낮추고... 확률 계산... 돌려준다. \n",
        "\n",
        "        optimizer.zero_grad() #초기화. \n",
        "\n",
        "        loss_w.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0: \n",
        "            print(\"Epoch : [{}/{}], Step : [{}/{}], Loss:{:.3f}\".format(epoch+1,num_epoch,i+1,total_step,loss_w.item()))\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : [1/5], Step : [100/600], Loss:0.230\n",
            "Epoch : [1/5], Step : [200/600], Loss:0.187\n",
            "Epoch : [1/5], Step : [300/600], Loss:0.191\n",
            "Epoch : [1/5], Step : [400/600], Loss:0.177\n",
            "Epoch : [1/5], Step : [500/600], Loss:0.244\n",
            "Epoch : [1/5], Step : [600/600], Loss:0.087\n",
            "Epoch : [2/5], Step : [100/600], Loss:0.087\n",
            "Epoch : [2/5], Step : [200/600], Loss:0.163\n",
            "Epoch : [2/5], Step : [300/600], Loss:0.160\n",
            "Epoch : [2/5], Step : [400/600], Loss:0.205\n",
            "Epoch : [2/5], Step : [500/600], Loss:0.232\n",
            "Epoch : [2/5], Step : [600/600], Loss:0.205\n",
            "Epoch : [3/5], Step : [100/600], Loss:0.186\n",
            "Epoch : [3/5], Step : [200/600], Loss:0.131\n",
            "Epoch : [3/5], Step : [300/600], Loss:0.237\n",
            "Epoch : [3/5], Step : [400/600], Loss:0.146\n",
            "Epoch : [3/5], Step : [500/600], Loss:0.152\n",
            "Epoch : [3/5], Step : [600/600], Loss:0.130\n",
            "Epoch : [4/5], Step : [100/600], Loss:0.095\n",
            "Epoch : [4/5], Step : [200/600], Loss:0.096\n",
            "Epoch : [4/5], Step : [300/600], Loss:0.153\n",
            "Epoch : [4/5], Step : [400/600], Loss:0.285\n",
            "Epoch : [4/5], Step : [500/600], Loss:0.212\n",
            "Epoch : [4/5], Step : [600/600], Loss:0.208\n",
            "Epoch : [5/5], Step : [100/600], Loss:0.274\n",
            "Epoch : [5/5], Step : [200/600], Loss:0.272\n",
            "Epoch : [5/5], Step : [300/600], Loss:0.256\n",
            "Epoch : [5/5], Step : [400/600], Loss:0.074\n",
            "Epoch : [5/5], Step : [500/600], Loss:0.143\n",
            "Epoch : [5/5], Step : [600/600], Loss:0.134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK967vuVaMNt"
      },
      "source": [
        "### 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuuoGUEIOL-b",
        "outputId": "c79295a8-1923-45d5-9c6a-4dafe2e35ae2"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    #평가를 하는 것이므로 경사구하는 것을 하지 않는다.\n",
        "    for images,labels in test_dataloader:\n",
        "        \n",
        "        image = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(image) #예측을 해본다. \n",
        "\n",
        "        _, predicted = torch.max(outputs.data,1) \n",
        "\n",
        "        #1의 의미. 각 '행'에서 최고인 값의 인덱스값을 주겠다는 의미가 된다.\n",
        "        #_은 value, predicted은 해당 value의 index값이 된다. \n",
        "        \"\"\" \n",
        "        a = torch.randn(4, 4)\n",
        "        >>>a\n",
        "        tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
        "                [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
        "                [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
        "                [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
        "        >>> torch.max(a, 1)\n",
        "        torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item() \n",
        "    print('Test Accuracy of the model on the 10000 test images:{}%'.format(100*correct /total))\n",
        "    torch.save(model.state_dict(), 'model.ckpt') \n",
        "\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images:90.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPMv-mcwOMBb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrI8p-EvOMEL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}