{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6baeb39",
   "metadata": {},
   "source": [
    "### Pytorch 에서의 Gradient Descent\n",
    "- 우리는 머신러닝에서 선형회귀(Linear Regression)에 대해서 간단히 살펴 보았습니다.\n",
    "- 이번에는 파이토치 라이브러리를 이용해서 선형회귀 연산 그래프를 만들고\n",
    "- 경사하강법을 이해하는 간단한 코드를 살펴보겠습니다.\n",
    "- 파이토치는 데이터의 기본단위로 텐서(Tensor)를 사용합니다.\n",
    "    - Numpy의 np.array()와 동일한 겁니다.\n",
    "- 텐서를 생성하는 방법에는 여러가지가 있지만 가장 단순한 방법으로 torch.Tensor()를 이용합니니다.\n",
    "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "- 베이직 코드라인이 위 Github 사이트 코드를 커스트마이징 했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3d31060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #파이토치 프레임워크 불러오고 ... 파이토치는 그냥 torch로 불러온다. \n",
    "import torchvision  #파이토치 내에서 vision이 붙으면... Image Processing에 틀화된 라이브러리\n",
    "import torch.nn as nn # nn은 Neural Net의 약자\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms # 데이타 변형에 특화된 라이브러리 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ecbb67ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.4936e+20, 1.9989e+20, 2.1867e+23],\n",
       "        [2.6583e-06, 4.0746e-11, 4.2330e+21]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= torch.Tensor(2,3) #2행 3열의 배열을 하나 생성... 이게 입력데이터가 된다. #토치타입의 텐서.\n",
    "X.shape #size(동일)\n",
    "X #값이 랜덤하게 초기화 되어서 들어가있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b8fbcd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랜덤하게 초기화되는게 맘에 들지 않는다면 생성과 동시에 초기화.\n",
    "X=torch.Tensor([[1,2,3],[4,5,6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc969c0",
   "metadata": {},
   "source": [
    "#### tensor 의 인자들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30b8e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data입력을 이렇게 키워드 변수로 명시할수도 있다.\n",
    "#device - (gpu, cpu 중 어느 것으로 돌릴 것인가.)\n",
    "#requires_grad - 텐서에 기울기를 적용할지 여부. 이게 없으면 미분 불가.\n",
    "\n",
    "#x = 2,3\n",
    "x = torch.tensor(data=[2.0,3.0],requires_grad=True) \n",
    "\n",
    "#y = 4,9\n",
    "y = x**2\n",
    "\n",
    "#pred = 11,21\n",
    "pred = 2*y +3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c2165a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<SumBackward0>)\n",
      "tensor([ 8., 12.]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-30e68d820edf>:26: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(x.grad,pred.grad)\n"
     ]
    }
   ],
   "source": [
    "#로스 구하기\n",
    "\n",
    "#label\n",
    "target = torch.tensor([3.0,4.0])\n",
    "\n",
    "#loss\n",
    "loss = torch.sum(torch.abs(pred-target)) # (11-3) + (21-4) =25\n",
    "print(loss)\n",
    "\n",
    "'''\n",
    "Forward 라인이 끝났다.\n",
    "X --> 학습 --> pred --> loss값 출력   \n",
    "\n",
    "Backward라인이 시작된다.\n",
    "loss.backward를 적용한다..\n",
    "\n",
    "'''\n",
    "\n",
    "loss.backward() #backward()가 학습 주체(weight, bias)에 대해서 미분이 진행된다. -> 로스를 낮추는 가장 빠른 루트를 찾는다.\n",
    "'''\n",
    "loss.backward()\n",
    "pred값은 미분대상이 아니다. None출력됨.\n",
    "\n",
    "'''\n",
    "\n",
    "print(x.grad,pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769ea2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Creating Tensor,  Neural Network Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56cb53e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7928,  1.0243, -0.8183],\n",
       "         [-0.6023,  0.9970, -0.2255],\n",
       "         [-1.5006,  0.1306,  1.5792],\n",
       "         [-0.6550, -0.3163, -1.1315],\n",
       "         [ 0.1642,  0.9358,  0.8347],\n",
       "         [-0.3821,  1.4423, -0.3366],\n",
       "         [ 1.5496,  0.2970,  1.0446],\n",
       "         [-2.3192,  0.8492,  0.5414],\n",
       "         [-0.9979, -1.4278, -0.2396],\n",
       "         [-0.4109, -0.8347,  0.5430]]),\n",
       " tensor([[ 2.5734, -1.5656],\n",
       "         [ 1.7894,  0.4850],\n",
       "         [-0.2732,  2.5391],\n",
       "         [ 0.9340,  0.2853],\n",
       "         [-0.8844, -0.0142],\n",
       "         [-1.0388,  0.9525],\n",
       "         [ 0.1871,  2.2987],\n",
       "         [-0.5476, -0.6009],\n",
       "         [ 0.6439,  0.5809],\n",
       "         [-0.3201, -0.0240]]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,3) #정규분포로 만들어준다\n",
    "y = torch.randn(10,2)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c7e7a",
   "metadata": {},
   "source": [
    "#### x가 10x3   y가 10x2... w의 행렬모양은? \n",
    "- y = x*w +b로 판단.. (3x2)모양이 될 것. 그러나..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dc54192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:  Parameter containing:\n",
      "tensor([[ 0.2850, -0.4702,  0.2208],\n",
      "        [ 0.2819, -0.5309,  0.0037]], requires_grad=True)\n",
      "bias:  Parameter containing:\n",
      "tensor([ 0.1655, -0.5022], requires_grad=True)\n",
      "<generator object Module.parameters at 0x00000247A4C2EDD0>\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn as nn     ->  nn은 Neural Net의 약자\n",
    "\n",
    "\n",
    "#weight, bias값 랜덤하게 생성\n",
    "linear = nn.Linear(3,2)\n",
    "print(\"weight: \",linear.weight)  #3x2가 아닌 2x3이 나왔다? ->바꾸어 주어야? No. 내부적으로 transpose\n",
    "print(\"bias: \",linear.bias) # 10x2가 아닌 1x2가 나왔다? 공통값을 부여해 예측의 오차를 줄이기 위함으로 보인다.\n",
    "\n",
    "#모델학습의 주체 -> weight,bias 해킹  (for문으로 내용확인 가능. generator형태이다. )\n",
    "print(linear.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84592a",
   "metadata": {},
   "source": [
    "#### 선 정의 해야하는 것\n",
    "- loss function\n",
    "    - MSELoss()\n",
    "- optimizer\n",
    "    - adaBoost\n",
    "    - SDG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95fb4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#로스펑션은 torch.nn이 갖고 있고 exponential, regularizaion 등은 적용되지 않는다. \n",
    "loss_func = nn.MSELoss() \n",
    "\n",
    "\n",
    "\n",
    "#옵티마이저는 torch.optim.SGD()  \n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr=0.1)\n",
    "\n",
    "#-인자는 위에서 weight와 bias를 해킹한 generator가 들어간다.\n",
    "#안에 들어있는 값이 하강할때 수정되는 값이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daa7dd",
   "metadata": {},
   "source": [
    "#### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4740e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성한 NN모델에 입력값을 넣으면 예측을 한다. \n",
    "pred = linear(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517fbe3",
   "metadata": {},
   "source": [
    "#### Loss구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e00513a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss... before BackPropagation:  2.3559229373931885\n"
     ]
    }
   ],
   "source": [
    "loss = loss_func(pred,y)\n",
    "print(\"Loss... before BackPropagation: \",loss.item())  #그냥 loss를 출력해도 되지만.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9967f5",
   "metadata": {},
   "source": [
    "#### Backward! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8dccc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[ 0.0491, -0.4137,  0.6026],\n",
      "        [ 0.7257, -0.5973, -0.7988]])\n",
      "dL/db tensor([-0.3712, -1.2826])\n"
     ]
    }
   ],
   "source": [
    "loss.backward() #실질적인 편미분 계산 \n",
    "print(\"dL/dw\", linear.weight.grad)\n",
    "print(\"dL/db\", linear.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ced84",
   "metadata": {},
   "source": [
    "#### 실질적인 하강... 위에서 얻어낸 값으로 step()을 밟는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1a40ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() #수정된 값으로 하강을 한다. #위에서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "348146ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after backpropagation 1.9992380142211914\n"
     ]
    }
   ],
   "source": [
    "#반복\n",
    "\n",
    "pred = linear(x)\n",
    "loss = loss_func(pred,y)\n",
    "print(\"loss after backpropagation\",loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c7bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57b468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
